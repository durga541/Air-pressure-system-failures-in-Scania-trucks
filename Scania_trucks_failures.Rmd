---
title: "Prediction of Air Pressure System (APS) Failures at Scania Trucks "
author: "Venkata Narasimha Durga Rao Pottella"
date: "11/21/2019"
output: pdf_document
---

# Executive summary:

The project report addresses the application of machine learning techniques in manufacturing and automobile industry. This project is based on the real time data obtained from sensors of the Scania trucks to predict the failures due to air pressure systems. 

The sensor data was given by experts and they didn’t mention any attribute names for proprietary reasons. So, we got the data with no attribute names from UCI machine learning repository. The data has 171 attributes lot of missing values, high correlation between the variables. There is also a cost matrix for misclassification and the goal is to reduce the cost.

We used unsupervised ML techniques like PCA to reduce the dimensionality and correlation between the variables. Then, we trained the data using 5 different advanced machine learning techniques like SVM with linear, radial and polynomial kernel, Random forest, XGBoost algorithm and calculated the cost based on the confusion matrix generated by the models. Out of all, XGBoost algorithm minimized the cost much lesser than the rest of the algorithms.

We can use this ML methods in the real time to produce the same results that we got here.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r Installing Libraries, results="hide", message=FALSE}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(ggplot2, naniar, foreach, iterators, itertools, missForest, Hmisc, lattice, survival, Formula,
               corrplot, ROSE, DMwR, randomForest, caret, mlbench, qcc, e1071, doParallel, xgboost, magrittr,
               Matrix, dplyr, parallelMap, parallel, gridExtra, knitr, rmarkdown)
registerDoParallel(cores=3)
```
# Objective:

The main purpose of this project is to classify the failures of the Scania trucks is whether by the components related to APS (Air pressure system) or by the other components that are not related to APS. The data has only 2 classes i.e. positive or negative. So, this is a binary classification problem.

There is also misclassification cost associated with this problem. 

```{r}
pred = c("positive", "negative")
act_post = c("-", "Cost_2")
act_neg = c("Cost_1,", "-")
mat = data.frame(pred, act_post,act_neg)
print(mat)
```
Here, Cost_1 refers to the cost that an unnecessary check needs to be done by a mechanic at the workshop and Cost_1 is 10, while Cost_2 refer to the cost of missing a faulty truck, which may cause a breakdown and Cost_2 is 500.

          Total_cost = Cost_1 x No_Instances + Cost_2 x No_Instances.

Here, our goal is to minimize the total cost by reducing the errors especially type 2 errors i.e false negatives.


# Description of the Dataset:

The dataset was founded in UCI machine learning repository. The source of the data was from Scania Trucks, Sweden. The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that are utilized in various functions in a truck, such as braking and gear changes. 

The dataset consists of 171 attributes for which names of them have been anonymized for
proprietary reasons. Out of those 171, one of the attributes is Class which consists of two variables i.e. positive or negative. The positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. 

The data obtained from UCI has two files, one is train and the other one is test. The train file consists of 60000 observations with 171 attributes. 70 of these attributes belong to 7 histograms with ten bins each. Out of the 60000 observations, 59000 belongs to negative class and 1000 belongs to positive class. Here, the ratio of negative to positive class is 59:1. This tells us that the negative class outnumbers positive class by a large proportion and the data also exhibits an unequal distribution between its classes. So, the dataset is imbalanced.  

# Loading the dataset:

```{r, echo=TRUE}
train = read.csv("aps_failure_training_set.csv", header = T, skip = 20, na.strings = 'na')
test = read.csv("aps_failure_test_set.csv", header = T, skip = 20, na.strings = 'na')
dim(train) # The size of train dataset 
dim(test) # The size of test dataset 
```

Now, let's look at the 171 attributes names. These attributes doesn't have any proper attrbute names since the data was extracted from sensors. The below are the names of 171 attributes.
 
```{r}
colnames(train)
```

# Exploratory Data Analysis: 

## Dealing with histograms and bins:

From the description of data, there are 70 attributes belong to 7 histograms with ten bins each. Let's first identify and deal with those 70 attributes. Adding up all the 10 variables in each bin across the columns and combining all 7 bins into dataframe and inspecting the head of the dataframe.

```{r}
bin1 = train[,c(8:17)]
bin2 = train[,c(34:43)]
bin3 = train[,c(44:53)]
bin4 = train[,c(54:63)]
bin5 = train[,c(101:110)]
bin6 = train[,c(115:124)]
bin7 = train[,c(160:169)]

b1 = rowSums(bin1)
b2 = rowSums(bin2)
b3 = rowSums(bin3)
b4 = rowSums(bin4)
b5 = rowSums(bin5)
b6 = rowSums(bin6)
b7 = rowSums(bin7)

bins = as.data.frame(cbind(b1, b2, b3, b4, b5, b6, b7))
head(bins)
```

Looks like all the 7 bins have same information. So, now replacing all the bins with single bin in the main dataset so that we can reduce the attributes which have redundant information. 

```{r}
colnames(bins) = c('bin')
his = train[,-c(8:17,34:63,101:110,115:124,160:169)]
truck = cbind(his,bins[1])
```

Now, the 171 attributes have reduced to 102 attributes.
 
```{r}
dim(truck)
```

The attribute "cd_000" has same values i.e 1209600 and there are some missing values. Since, there is no variation in this attribute, we are removing this attribute from the dataset. The below is the summary of this:

```{r}
summary(train$cd_000)
truck = truck[-51]
```

## Plotting graphs:

As the attributes doesn't have any names, we cannot infer anything from graphs. But, let's see how the class variable is distributed w.r.t various attributes.

```{r}
p1 = ggplot(truck, aes(bin, ah_000, colour = class)) + geom_point()
p2 = ggplot(truck, aes(bin, bi_000, colour = class)) + geom_point()
p3 = ggplot(truck, aes(bin, ci_000, colour = class)) + geom_point()
p4 = ggplot(truck,aes(bin, ds_000, colour = class)) + geom_point()
p5 = ggplot(truck,aes(bin, eb_000, colour = class)) + geom_point()
p6 = ggplot(truck,aes(ap_000, ck_000, colour = class)) + geom_point()
grid.arrange(p1,p2,p3,p4,p5,p6,ncol = 2, nrow = 3)
```

## Dealing with missing values:

The data has lot of missing values of 0.2% to upto 82%. we choose to impute them with median of the corresponding variables as the data is from sensors and doesn't have any attribute names. The below plot depicts the missing values of each variables.

```{r}
gg_miss_var(truck)
```

This shows clearly which attributes have how many missing values.As, we can see the attributes "br_000", "bq_000", "bp_000", "bo_000", "bn_000" have more that 70% of the missing values. Out of 101 attributes, 99 attributes have missing values. 

```{r}
colSums(is.na(truck))
```

Now, After imputing the missing the values with median of the respective columns, there are no missing values in the dataset.

```{r}
train.imp = truck[,-c(1,2)]
for(i in 1:ncol(train.imp)){train.imp[is.na(train.imp[,i]), i] <- median(train.imp[,i], na.rm = TRUE)}
aps_train = cbind(truck[,c(1,2)], train.imp)
gg_miss_var(aps_train)
```

## Correlation Analysis:

Let’s look at the correlation between the attributes. The below plot depicts the correlation between the variables. From the plot, we can see that there is either zero or positive correlation between the variables.

```{r}
corrplot(cor(aps_train[,-1]), type = "lower", method = "ellipse")
```

Among all the attributes, let us see the most correlated attributes i.e the correlation above 0.75.

```{r}
correlationMatrix <- cor(aps_train[,-1])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75,names = TRUE)
print(highlyCorrelated)

```

# Data Preprocessing:

## Principal Component Analysis:
We have lot of attributes i.e 101 and lot of correlated variables. So, the best feature engineering is Prinicpal Component Analysis(PCA).

```{r}
pri = prcomp(aps_train[,-1], scale. = T, center = T)
summary(pri)
```

The summary describes the importance of the PCs. The first row describe the standard deviation associated with each PC. The second row shows the proportion of the variance in the data explained by each component while the third row describe the cumulative proportion of explained variance. We can see there that the 42 PCs accounts for more than 90% of the variance of the data and 52 PCs accounts for 95% of the variance of the data.

```{r}
cov = round(pri$sdev^2/sum(pri$sdev^2)*100, 2)
cov = data.frame(c(1:100),cov)
names(cov)[1] = 'PCs'
names(cov)[2] = 'Variance'
ggplot(cov, aes(PCs, Variance)) +geom_line(color="orange",size=0.8) + geom_point() + ggtitle("Variance captured by each PC")

```

The above plot shows the variance captured by each PC. The plot depicts decrease in variation captured when we move to the extreme end as in PCA, the initial PCs capture lot of variation. 

```{r,results="hide"}
PCA = pri$sdev^2
names(PCA) = paste0('PC', cov$PCs)
pareto.chart(PCA)
pca.data = as.data.frame(pri$x)
pca.data = cbind(pca.data, aps_train[1])
pca.best  = pca.data[,c(1:52,101)]
```

Since, 95% of the variation is capured by 52 PCs, we are using only 52 PCs as the independent variables for further analysis.

## Preprocessing the test dataset:

Preforming all the above operations on test dataset also for evaluating and calculating the cost matrices. We imputed the missing values with median for test dataset also. 

```{r}
bin1_tt = test[,c(8:17)]
bin2_tt = test[,c(34:43)]
bin3_tt = test[,c(44:53)]
bin4_tt = test[,c(54:63)]
bin5_tt = test[,c(101:110)]
bin6_tt = test[,c(115:124)]
bin7_tt = test[,c(160:169)]

b1_rst = rowSums(bin1_tt)
b2_rst = rowSums(bin2_tt)
b3_rst = rowSums(bin3_tt)
b4_rst = rowSums(bin4_tt)
b5_rst = rowSums(bin5_tt)
b6_rst = rowSums(bin6_tt)
b7_rst = rowSums(bin7_tt)

bins_test = as.data.frame(cbind(b1_rst, b2_rst, b3_rst, b4_rst, b5_rst, b6_rst, b7_rst))
colnames(bins_test) = c('bin')
his_test = test[,-c(8:17,34:63,91,101:110,115:124,160:169)]

truck_test = cbind(his_test,bins_test[1])

test.imp = truck_test[,-c(1,2)]
for(i in 1:ncol(test.imp)){test.imp[is.na(test.imp[,i]), i] <- median(test.imp[,i], na.rm = TRUE)}
aps_test = cbind(truck_test[,c(1,2)], test.imp)
gg_miss_var(aps_test)
```

Using PCA of training set, predicting the PCA values for test dataset for evaluating in conjuction with train dataset.

```{r}
pca.test.best = predict(pri, aps_test[-1])
pca.test = as.data.frame(cbind(pca.test.best, aps_test[1]))
pca.test = pca.test[,c(1:52,101)]
```

## Dealing wth unbalanced data:

```{r}
table(pca.best$class)
```

Since the data is unbalanced, working on this will lead to reduction in accuracy and the models will get biased towards majority class. So, to balance the dataset, we considered the below 3 methods.
•	Under Sampling
•	Over Sampling
•	Synthetic minority oversampling technique (SMOTE) 
• Both Under and Over Sampling
The data is modified using these 4 methods and can be tested on logisitc regression to test the best technique for further ananlysis.

### Over Sampling:
```{r}
data.balanced.over <- ovun.sample(class~., data=pca.best, 
                                  p=0.7, seed=1, 
                                  method="over")$data
table(data.balanced.over$class)
logit.over <- glm(class~., data = data.balanced.over, family = "binomial") 
logit.over.pred <- predict(logit.over, data.balanced.over, type = "response")
over.pred <- as.data.frame(ifelse(logit.over.pred > 0.5,"pos","neg"))
names(over.pred) = c("class")
confusionMatrix(factor(over.pred$class),factor(data.balanced.over$class))
```
Here the accuracy is 0.9232 or 92.32%.


### Under Sampling:
```{r}
data.balanced.under <- ovun.sample(class~., data=pca.best, 
                                  p=0.5, seed=1, 
                                  method="under")$data
table(data.balanced.under$class)
logit.under <- glm(class~., data = data.balanced.under, family = "binomial") 
logit.under.pred <- predict(logit.under, data.balanced.under, type = "response")
under.pred <- as.data.frame(ifelse(logit.under.pred > 0.5,"pos","neg"))
names(under.pred) = c("class")
confusionMatrix(factor(under.pred$class),factor(data.balanced.under$class))
```
Here the accuracy is 0.8929 or 89.29%.

### SMOTE:
```{r}
data.smote = SMOTE(class~., pca.best, perc.over = 1900, perc.under = 210.53, k=5)

table(data.smote$class)
logit.smote <- glm(class~., data = data.smote, family = "binomial") 
logit.smote.pred <- predict(logit.smote, data.smote, type = "response")
smote.pred <- as.data.frame(ifelse(logit.smote.pred > 0.5,"pos","neg"))
names(smote.pred) = c("class")
confusionMatrix(factor(smote.pred$class),factor(data.smote$class))
```
Here the accuracy is 0.946 or 94.6%.

### Both(Under and Over Sampling):
```{r}
data.both = ovun.sample(class~., data=pca.best, p=0.5, seed=1, 
                        method="both")$data
table(data.both$class)
logit.both <- glm(class~., data = data.both, family = "binomial") 
logit.both.pred <- predict(logit.both, data.both, type = "response")
both.pred <- as.data.frame(ifelse(logit.both.pred > 0.5,"pos","neg"))
names(both.pred) = c("class")
confusionMatrix(factor(both.pred$class),factor(data.both$class))
```
Here the accuracy is 0.9257 or 92.57%

So, the best technique for this unbalanced dataset is SMOTE technique which generates the neg to pos ratio of 2:1. we will use SMOTE technique for further analysis.

```{r}
pca.best.smote = SMOTE(class~., pca.best, perc.over = 1900, perc.under = 210.53, k=5)
table(pca.best.smote$class)
```

# Data Modelling:

Here, our goal is to calculate the cost associated with missclassification errors and find the best best model with minimum cost. So for calculating the cost, we are generating confusion matrix for each model and comparing them at the end.

## SVM with Linear Kernel:
```{r}
svm.lin <- svm(class~., data=pca.best.smote, kernel='linear', cost=0.01)
summary(svm.lin)
```

From summary above, we found out that there are total of 9069 support vectors with 4531 for 'neg' class and 4538 for 'pos' class.

```{r}
pred_lin <- predict(svm.lin, pca.test)
confusionMatrix(pred_lin, pca.test$class)
```

The accuracy for test dataset for SVM model with linear kernel with cost=0.01 is 0.9725 or 97.25%. So, the error rate is 0.0275 or 2.75% which is less than the naive classification accuracy of 97.65%. The cost obtained from this model is 

Total Cost = 408 x 10 + 32 x 500 = 20,080

## SVM with Radial Kernel:

```{r}
svm.rad <- svm(class~., data=pca.best.smote, kernel='radial', gamma=0.1, cost=0.1)
summary(svm.rad)
```

From summary above, we found out that there are total of 10168 support vectors with 4258 for 'neg' class and 5910 for 'pos' class.

```{r}
pred_rad <- predict(svm.rad, pca.test)
confusionMatrix(pred_rad, pca.test$class)
```

The accuracy for test dataset for SVM model with radial kernel with cost=0.1 is 0.9597 or 95.97%. So, the error rate is 0.0403 or 4.03% which is less than the naive classification accuracy of 97.65%. The cost obtained from this model is 

Total Cost = 626 x 10 + 19 x 500 = 15,760


## SVM with Polynomial Kernel:
```{r}
svm.ploy <- svm(class~., data=pca.best.smote, kernel='polynomial', cost=0.01,gamma=0.1, degree=2)
summary(svm.ploy)

```
From summary above, we found out that there are total of 12870 support vectors with 6452 for 'neg' class and 6418 for 'pos' class.

```{r}
pred_ploy <- predict(svm.ploy, pca.test)
confusionMatrix(pred_ploy, pca.test$class)
```
The accuracy for test dataset for SVM model with polynomial kernel with cost=0.01 is 0.9812 or 98.12%. So, the error rate is 0.0188 or 1.88% which is higher than the naive classification accuracy of 97.65%. The cost obtained from this model is 

Total Cost = 225 x 10 + 75 x 500 = 39,750

Here, the cost is high compared to other SVM models.

## Random Forest:
Now, we will create a Random Forest model with default parameters i.e Ntree i.e Number of trees to grow is 500 and number of variables tried at each split i.e mtry is 6 in this case. Error rate is 0.86%.

```{r}
set.seed(1111)
rf = randomForest(class~., data = pca.best.smote, ntree = 500, mtry = 6, importance= TRUE)
rf
```


```{r}
pred_rf <- predict(rf, pca.test, type = "class")
confusionMatrix(pred_rf, pca.test$class)
```
In case of prediction on test dataset, there is 0.0183 or 1.83% misclassification and 293 data points are misclassified and accuracy is 98.17% which is higher than the naive classification accuracy of 97.65%. The cost obtained from this model is 

Total Cost = 249 x 10 + 44 x 500 = 24,490

Seems like SVM outperformed Random forest for this dataset w.r.t cost.

```{r}
oob.error.data <- data.frame(
  Trees=rep(1:nrow(rf$err.rate), times=3),
  Type=rep(c("OOB", "neg", "pos"), each=nrow(rf$err.rate)),
  Error=c(rf$err.rate[,"OOB"], 
    rf$err.rate[,"neg"], 
    rf$err.rate[,"pos"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type)) + ggtitle("")
```
From the above plot, we can see that the error decreases as the number of trees increases. Here the error is high for "pos" class compared to "neg" class since the majority class in the data is "neg" class.

## XGBoost Algorithm:

Now, we will train a XGBoost Algorithm with default parameters i.e eta(learning rate) is 0.1, max_depth that the trees will be generated is 6, no of iterations is 100. The objective function used here is mutli:softmax which is multiclassification using softmax objective and returns predicted class labels.The evaluation metrics is mlogloss which is multiclass logloss used in classification.

```{r, echo=TRUE}
#pca.best.smote$class <- as.numeric(as.factor(pca.best.smote$class)) - 1
train_label = pca.best.smote[,'class']
train_label = as.numeric(as.factor(train_label)) - 1
train_matrix = xgb.DMatrix(data = as.matrix(pca.best.smote[-53]), label=train_label)


test_label = pca.test[,'class']
test_label = as.numeric(as.factor(test_label)) - 1
test_matrix = xgb.DMatrix(data = as.matrix(pca.test[-53]), label=test_label)

```

```{r, results="hide", echo=TRUE}
nc = length(unique(train_label))
xgb_params = list("objective" == "binary:logistic",
                  "eval_metric" == "mlogloss",
                  "num_class" == "nc")
watchlist = list(train = train_matrix, test= test_matrix)
set.seed(1111)
xgb <- xgboost(data = train_matrix, 
               eta = 0.1,
               max_depth = 6, 
               nround=100, 
               subsample = 0.5,
               colsample_bytree = 0.5,
               seed = 1111, gamma = 100,
               eval_metric = "mlogloss",
               objective = "multi:softmax",
               num_class = 2,
               nthread = 3)

```

```{r, echo=TRUE}
e <- data.frame(xgb$evaluation_log)
plot(e$iter, e$train_mlogloss, col = 'blue')

```
From the above plot, we can see that the error is reducing as no of iterations increses. 
```{r}
# Prediction & confusion matrix - test data
y_pred <- predict(xgb, data.matrix(pca.test[,-53]))

y_pred = as.data.frame(y_pred)
y_pred[y_pred == 0] <- c("neg")
y_pred[y_pred == 1] <- c("pos")
confusionMatrix(factor(y_pred$y_pred), factor(pca.test$class))
```
In case of prediction on test dataset, there is 0.0443 or 4.43% misclassification and 709 data points are misclassified and accuracy is 95.57% which is less than the naive classification accuracy of 97.65%. The cost obtained from this model is 

Total Cost = 695 x 10 + 14 x 500 = 13,950

Here the accuracy is less, but this algorithm reduced false negatives errors significantly. Thus we got the low cost compared to all the models.

# Conclusions:

Dealing with a dataset with no attribute names is quite a difficult task in terms of understanding the data w.r.t the target variable. But, with a handful of latest machine learning algorithms, we can find a way to deal with this kind of datasets especially through ensemble techniques. 

In this case, we used PCA to reduce the dimensionality and worked on 5 different advanced machine learning algorithms and here our goal is to minimize the cost associated with missclassification. So, in terms of the cost, the best algorithm we found for this data is XGBoost algorithm with a minimum cost of 13,950.

# Future Approach:

We can try different other algorithms with advaced techniques like bagging and pasting etc and we can fine tune the hyperparameters in the best possible way to get less minimum cost. Here, due to long running algorithms for optimizing hyperparameters, we didn't go for optimizing the hyper parameters. 


